---
title:          "Training-free and Adaptive Sparse Attention for Efficient Long Video Generation"
date:           2025-01-09 00:00:00 +0800
selected:       false
pub:            "<strong><span style='color: blue'>[ICCV 2025 (CCF-A) | Fourth Author]</span></strong> International Conference on Computer Vision"
pub_date:       "2025"

abstract: >-
  Generating high-fidelity long videos with Diffusion Transformers (DiTs) is often hindered by significant latency, primarily due to the computational demands of attention mechanisms. We propose AdaSpa, the first Dynamic Pattern and Online Precise Search sparse attention method. Firstly, to realize the Dynamic Pattern, we introduce a blockified pattern to efficiently capture the hierarchical sparsity inherent in DiTs. Secondly, to enable Online Precise Search, we propose the Fused LSE-Cached Search with Head-adaptive Hierarchical Block Sparse Attention. Experiments validate AdaSpa's substantial acceleration while preserving video quality.
cover:          /assets/images/covers/AdaSpa.png
authors:  
  - Yifei Xia
  - Suhan Ling
  - Fangcheng Fu
  - <strong><u>Yujie Wang</u></strong>
  - Huixia Li
  - Xuefeng Xiao
  - Bin Cui
links:
  Arxiv: https://arxiv.org/abs/2502.21079
---
