---
title:          "Improving Automatic Parallel Training via Balanced Memory Workload Optimization"
date:           2024-08-31 00:00:00 +0800
selected:       true
pub:            "<strong>[TKDE 2024 | First Author]</strong> IEEE Transactions on Knowledge and Data Engineering"
pub_date:       "2024"

abstract: >-
  Efficiently training Transformer models across multiple GPUs remains a complex challenge due to the abundance of parallelism options. In this paper, we present Galvatron-BMW, a novel system framework that integrates multiple prevalent parallelism dimensions, which not only targets automatic parallelism optimization for large-scale Transformer models training, but also considers the Balancing trade-off between Memory and computation Workloads across devices through a novel bi-objective optimization framework. Experiments demonstrate the efficiency of our system.
cover:          /assets/images/covers/Galvatron-BMW.png
authors:
  - <strong><u>Yujie Wang</u></strong>
  - Youhe Jiang
  - Xupeng Miao
  - Fangcheng Fu
  - Shenhan Zhu
  - Xiaonan Nie
  - Yaofeng Tu
  - Bin Cui
links:
  Paper: https://dl.acm.org/doi/10.1109/TKDE.2024.3370614
  Arxiv: https://arxiv.org/abs/2307.02031
  Code: https://github.com/PKU-DAIR/Hetu-Galvatron
---
